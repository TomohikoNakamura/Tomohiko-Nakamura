
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <meta name="author" content="Tomohiko Nakamura">
      
      
        <link rel="canonical" href="https://tomohikonakamura.github.io/Tomohiko-Nakamura/publications.html">
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.1.5">
    
    
      
        <title>Publications / 発表文献 - Tomohiko Nakamura</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.bde7dde4.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.ef6f36e2.min.css">
        
          
          
          <meta name="theme-color" content="#757575">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="stylesheets/extra.css">
    
    
      
        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9EPRVMYY0Z"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9EPRVMYY0Z');
</script>
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="grey" data-md-color-accent="primary">
  
    
    <script>function __prefix(e){return new URL(".",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#publications" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="Tomohiko Nakamura" class="md-header__button md-logo" aria-label="Tomohiko Nakamura" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Tomohiko Nakamura
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Publications / 発表文献
            
          </span>
        </div>
      </div>
    </div>
    
    
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="index.html" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="research.html" class="md-tabs__link">
      Research / Demo
    </a>
  </li>

      
        
  
  
    
  


  <li class="md-tabs__item">
    <a href="publications.html" class="md-tabs__link md-tabs__link--active">
      Publications
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="lecture.html" class="md-tabs__link">
      Lecture Notes
    </a>
  </li>

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="Tomohiko Nakamura" class="md-nav__button md-logo" aria-label="Tomohiko Nakamura" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Tomohiko Nakamura
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="research.html" class="md-nav__link">
        Research / Demo
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="publications.html" class="md-nav__link md-nav__link--active">
        Publications
      </a>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="lecture.html" class="md-nav__link">
        Lecture Notes
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <p><strong><a href="https://scholar.google.com/citations?user=bHny6PAAAAAJ">Google Scholar<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></span></a> / <a href="https://researchmap.jp/tomohiko_nakamura">Researchmap<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></span></a> / <a href="https://orcid.org/0000-0003-4385-7170">ORCiD<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></span></a></strong></p>
<h1 id="publications">Publications / 発表文献<a class="headerlink" href="#publications" title="Permanent link">&para;</a></h1>
<h2>Journals / 査読付き論文誌</h2>
<ol>
<li><ins>Tomohiko Nakamura</ins>, Shihori Kozuka, and Hiroshi Saruwatari, “<strong>Time-Domain Audio Source Separation with Neural Networks Based on Multiresolution Analysis</strong>,” <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 1687–1701, Apr. 2021. <a href="https://doi.org/10.1109/TASLP.2021.3072496"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="demo/MRDLA/">demo<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M470.38 1.51 150.41 96A32 32 0 0 0 128 126.51v261.41A139 139 0 0 0 96 384c-53 0-96 28.66-96 64s43 64 96 64 96-28.66 96-64V214.32l256-75v184.61a138.4 138.4 0 0 0-32-3.93c-53 0-96 28.66-96 64s43 64 96 64 96-28.65 96-64V32a32 32 0 0 0-41.62-30.49z"/></svg></span></a>, <a href="https://github.com/TomohikoNakamura/dwtls">code<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins> and Hirokazu Kameoka, “<strong>Harmonic-Temporal Factor Decomposition for Unsupervised Monaural Separation of Harmonic Sounds</strong>,” <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 68–82, Nov. 2020. <a href="https://doi.org/10.1109/TASLP.2020.3037487"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="demo/HTFD/">demo<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M470.38 1.51 150.41 96A32 32 0 0 0 128 126.51v261.41A139 139 0 0 0 96 384c-53 0-96 28.66-96 64s43 64 96 64 96-28.66 96-64V214.32l256-75v184.61a138.4 138.4 0 0 0-32-3.93c-53 0-96 28.66-96 64s43 64 96 64 96-28.65 96-64V32a32 32 0 0 0-41.62-30.49z"/></svg></span></a>, <a href="https://github.com/TomohikoNakamura/HTFD">code<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins>, Eita Nakamura, and Shigeki Sagayama, “<strong>Real-Time Audio-to-Score Alignment of Music Performances Containing Errors and Arbitrary Repeats and Skips</strong>,” <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 24, no. 2, pp. 329–339, Feb. 2016. <a href="https://doi.org/10.1109/TASLP.2015.2507862"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="http://arxiv.org/abs/1512.07748"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="demo/automatic_accompaniment/">demo<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M470.38 1.51 150.41 96A32 32 0 0 0 128 126.51v261.41A139 139 0 0 0 96 384c-53 0-96 28.66-96 64s43 64 96 64 96-28.66 96-64V214.32l256-75v184.61a138.4 138.4 0 0 0-32-3.93c-53 0-96 28.66-96 64s43 64 96 64 96-28.65 96-64V32a32 32 0 0 0-41.62-30.49z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins>, Yutaka Hori, and Shinji Hara, “<strong>Hierarchical Modeling and Local Stability Analysis for Repressilators Coupled by Quorum Sensing</strong>,” <em>SICE Journal of Control, Measurement, and System Integration</em>, vol. 7, no. 3, pp. 133–140, May 2014. <a href="https://doi.org/10.9746/jcmsi.7.133"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <span style="color: var(--md-code-hl-function-color)">[SICE Best Paper Award (Takeda Award) / 2015年計測自動制御学会 論文賞 (武田賞)]</span></li>
<li>Eita Nakamura, <ins>Tomohiko Nakamura</ins>, Yasuyuki Saito, Nobutaka Ono, and Shigeki Sagayama, “<strong>Outer-Product Type Hidden Markov Model and Polyphonic MIDI Score Following</strong>,” <em>Journal of New Music Research</em>, vol. 43, pp. 183–201, Apr. 2014. <a href="https://doi.org/10.1080/09298215.2014.884145"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="http://arxiv.org/abs/1404.2313"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
</ol>
<h2>International Conferences / 査読付き国際会議</h2>
<ol>
<li>Takuya Hasumi, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, Hiroshi Saruwatari, Daichi Kitamura, Yu Takahashi, and Kazunobu Kondo, “<strong>Multichannel Audio Source Separation with Independent Deeply Learned Matrix Analysis Using Product of Source Models</strong>,” in <em>Proceedings of the 13<sup>th</sup> Asia Pacific Signal and Information Processing Association Annual Summit and Conference</em>, Dec. 2021. (to appear) <a href="https://arxiv.org/abs/2109.00704"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li>Sota Misawa, Norihiro Takamune, <ins>Tomohiko Nakamura</ins>, Daichi Kitamura, Hiroshi Saruwatari, Masakazu Une, and Shoji Makino, “<strong>Speech Enhancement by Noise Self-Supervised Rank-Constrained Spatial Covariance Matrix Estimation via Independent Deeply Learned Matrix Analysis</strong>,” in <em>Proceedings of the 13<sup>th</sup> Asia Pacific Signal and Information Processing Association Annual Summit and Conference</em>, Dec. 2021. (to appear) <a href="https://arxiv.org/abs/2109.04658"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li>Yusaku Mizobuchi, Daichi Kitamura, <ins>Tomohiko Nakamura</ins>, Hiroshi Saruwatari, Yu Takahashi, and Kazunobu Kondo, “<strong>Prior Distribution Design for Music Bleeding-Sound Reduction Based on Nonnegative Matrix Factorization</strong>,” in <em>Proceedings of the 13<sup>th</sup> Asia Pacific Signal and Information Processing Association Annual Summit and Conference</em>, Dec. 2021. (to appear) <a href="https://arxiv.org/abs/2109.00237"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li>Koichi Saito, <ins>Tomohiko Nakamura</ins>, Kohei Yatabe, Yuma Koizumi, and Hiroshi Saruwatari, “<strong>Sampling-Frequency-Independent Audio Source Separation Using Convolution Layer Based on Impulse Invariant Method</strong>,” in <em>Proceedings of the 29<sup>th</sup> European Signal Processing Conference</em>, Aug. 2021, pp. 321–325. <a href="https://arxiv.org/abs/2105.04079"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li>Naoki Narisawa, Rintaro Ikeshita, Norihiro Takamune, Daichi Kitamura, <ins>Tomohiko Nakamura</ins>, Hiroshi Saruwatari, and Tomohiro Nakatani, “<strong>Independent Deeply Learned Tensor Analysis for Determined Audio Source Separation</strong>,” in <em>Proceedings of the 29<sup>th</sup> European Signal Processing Conference</em>, Aug. 2021, pp. 326–330. <a href="https://arxiv.org/abs/2106.05529"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li>Takuya Hasumi, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, Hiroshi Saruwatari, Daichi Kitamura, Yu Takahashi, and Kazunobu Kondo, “<strong>Empirical Bayesian Independent Deeply Learned Matrix Analysis for Multichannel Audio Source Separation</strong>,” in <em>Proceedings of the 29<sup>th</sup> European Signal Processing Conference</em>, Aug. 2021, pp. 331–335. <a href="https://arxiv.org/abs/2106.03492"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li>Shihori Kozuka, <ins>Tomohiko Nakamura</ins>, and Hiroshi Saruwatari, “<strong>Investigation on Wavelet Basis Function of DNN-Based Time Domain Audio Source Separation Inspired by Multiresolution Analysis</strong>,” in <em>Proceedings of the 49<sup>th</sup> International Congress and Exposition on Noise Control Engineering</em>, Aug. 2020, pp. 4013–4022. <a href="https://www.ingentaconnect.com/contentone/ince/incecp/2020/00000261/00000002/art00004"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins> and Hiroshi Saruwatari, “<strong>Time-Domain Audio Source Separation Based on Wave-U-Net Combined with Discrete Wavelet Transform</strong>,” in <em>Proceedings of the 45<sup>th</sup> International Conference on Acoustics, Speech, and Signal Processing</em>, May 2020, pp. 386–390. <a href="https://doi.org/10.1109/ICASSP40776.2020.9053934"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="https://arxiv.org/abs/2001.10190"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins> and Hirokazu Kameoka, “<strong>Shifted and Convolutive Source-Filter Non-Negative Matrix Factorization for Monaural Audio Source Separation</strong>,” in <em>Proceedings of the 41<sup>st</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</em>, Mar. 2016, pp. 489–493. <a href="https://doi.org/10.1109/ICASSP.2016.7471723"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins> and Hirokazu Kameoka, “<strong>Lp-Norm Non-Negative Matrix Factorization and Its Application to Singing Voice Enhancement</strong>,” in <em>Proceedings of 2015 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, Apr. 2015, pp. 2115–2119. <a href="https://doi.org/10.1109/ICASSP.2015.7178344"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins>, Kotaro Shikata, Norihiro Takamune, and Hirokazu Kameoka, “<strong>Harmonic-Temporal Factor Decomposition Incorporating Music Prior Information for Informed Monaural Source Separation</strong>,” in <em>Proceedings of the 15<sup>th</sup> International Society for Music Information Retrieval Conference</em>, Oct. 2014, pp. 623–628. <a href="http://www.terasoft.com.tw/conf/ismir2014/proceedings/T112_135_Paper.pdf"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="demo/HTFD">demo<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M470.38 1.51 150.41 96A32 32 0 0 0 128 126.51v261.41A139 139 0 0 0 96 384c-53 0-96 28.66-96 64s43 64 96 64 96-28.66 96-64V214.32l256-75v184.61a138.4 138.4 0 0 0-32-3.93c-53 0-96 28.66-96 64s43 64 96 64 96-28.65 96-64V32a32 32 0 0 0-41.62-30.49z"/></svg></span></a>, <span style="color: var(--md-code-hl-function-color)">[Travel Grant by the Tateishi Science and Technology Foundation]</span></li>
<li><ins>Tomohiko Nakamura</ins> and Hirokazu Kameoka, “<strong>Fast Signal Reconstruction from Magnitude Spectrogram of Continuous Wavelet Transform Based on Spectrogram Consistency</strong>,” in <em>Proceedings of the 17<sup>th</sup> International Conference on Digital Audio Effects</em>, Sep. 2014, pp. 129–135. <a href="http://www.dafx14.fau.de/papers/dafx14_tomohiko_nakamura_fast_signal_reconstructio.pdf"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="demo/fastCWT/">demo<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M470.38 1.51 150.41 96A32 32 0 0 0 128 126.51v261.41A139 139 0 0 0 96 384c-53 0-96 28.66-96 64s43 64 96 64 96-28.66 96-64V214.32l256-75v184.61a138.4 138.4 0 0 0-32-3.93c-53 0-96 28.66-96 64s43 64 96 64 96-28.65 96-64V32a32 32 0 0 0-41.62-30.49z"/></svg></span></a>, <span style="color: var(--md-code-hl-function-color)">[Travel Grant by the Hara Research Foundation]</span></li>
<li>Takuya Higuchi, Hirofumi Takeda, <ins>Tomohiko Nakamura</ins>, and Hirokazu Kameoka, “<strong>A Unified Approach for Underdetermined Blind Signal Separation and Source Activity Detection by Multichannel Factorial Hidden Markov Models</strong>,” in <em>Proceedings of the 15<sup>th</sup> Annual Conference of the International Speech Communication Association</em>, Sep. 2014, pp. 850–854. <a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2014/i14_0850.pdf"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins>, Hirokazu Kameoka, Kazuyoshi Yoshii, and Masataka Goto, “<strong>Timbre Replacement of Harmonic and Drum Components for Music Audio Signals</strong>,” in <em>Proceedings of 2014 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, May 2014, pp. 7520–7524. <a href="https://doi.org/10.1109/ICASSP.2014.6855052"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="demo/drum_timbre_replacement/">demo<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M470.38 1.51 150.41 96A32 32 0 0 0 128 126.51v261.41A139 139 0 0 0 96 384c-53 0-96 28.66-96 64s43 64 96 64 96-28.66 96-64V214.32l256-75v184.61a138.4 138.4 0 0 0-32-3.93c-53 0-96 28.66-96 64s43 64 96 64 96-28.65 96-64V32a32 32 0 0 0-41.62-30.49z"/></svg></span></a></li>
<li>Takuya Higuchi, Norihiro Takamune, <ins>Tomohiko Nakamura</ins>, and Hirokazu Kameoka, “<strong>Underdetermined Blind Separation and Tracking of Moving Sources Based on DOA-HMM</strong>,” in <em>Proceedings of 2014 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, May 2014, pp. 3215–3219. <a href="https://doi.org/10.1109/ICASSP.2014.6854189"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li>Shigeki Sagayama, <ins>Tomohiko Nakamura</ins>, Eita Nakamura, Yasuyuki Saito, Hirokazu Kameoka, and Nobutaka Ono, “<strong>Automatic Music Accompaniment Allowing Errors and Arbitrary Repeats and Jumps</strong>,” in <em>Proceedings of Meetings on Acoustics, Acoustic Society of America</em>, May 2014, vol. 21, 35003. <a href="https://doi.org/10.1121/1.4877848"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins>, Eita Nakamura, and Shigeki Sagayama, “<strong>Acoustic Score Following to Musical Performance with Errors and Arbitrary Repeats and Skips for Automatic Accompaniment</strong>,” in <em>Proceedings of Sound and Music Computing Conference</em>, Aug. 2013, pp. 299–304. <a href="http://smcnetwork.org/node/1754"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="demo/automatic_accompaniment">demo<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M470.38 1.51 150.41 96A32 32 0 0 0 128 126.51v261.41A139 139 0 0 0 96 384c-53 0-96 28.66-96 64s43 64 96 64 96-28.66 96-64V214.32l256-75v184.61a138.4 138.4 0 0 0-32-3.93c-53 0-96 28.66-96 64s43 64 96 64 96-28.65 96-64V32a32 32 0 0 0-41.62-30.49z"/></svg></span></a>, <span style="color: var(--md-code-hl-function-color)">[Travel Grant by the Telecommunications Advancement Foundation]</span></li>
<li>Masahiro Nakano, Jonathan Le Roux, Hirokazu Kameoka, <ins>Tomohiko Nakamura</ins>, Nobutaka Ono, and Shigeki Sagayama, “<strong>Bayesian Nonparametric Spectrogram Modeling Based on Infinite Factorial Infinite Hidden Markov Model</strong>,” in <em>Proceedings of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</em>, Oct. 2011, pp. 325–328. <a href="https://doi.org/10.1109/ASPAA.2011.6082324"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a></li>
<li><ins>Tomohiko Nakamura</ins>, Shinji Hara, and Yutaka Hori, “<strong>Local Stability Analysis for a Class of Quorum-Sensing Networks with Cyclic Gene Regulatory Networks</strong>,” in <em>Proceedings of SICE Annual Conference</em>, Sep. 2011, pp. 2111–2116. <a href="https://ieeexplore.ieee.org/document/6060320"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <span style="color: var(--md-code-hl-function-color)">[SICE Annual Conference 2011 International Award and Finalist of Young Author's Award]</span></li>
</ol>
<h2>Domestic Conferences / 国内会議</h2>
<ol>
<li>溝渕 悠朔, 北村 大地, <ins>中村 友彦</ins>, 猿渡 洋, 高橋 祐,  近藤 多伸, “<strong>非負値行列因子分解を用いた被り音の抑圧</strong>,” <em>情報処理学会研究報告</em>, vol. 2021–MUS–132, no. 24, Sep. 2021.</li>
<li>川村 真也, <ins>中村 友彦</ins>, 北村 大地, 猿渡 洋, 高橋 祐,  近藤 多伸, “<strong>楽譜情報を援用した音楽音響信号に対する混合differentiable DSPモデルの合成パラメータ推定</strong>,” <em>情報処理学会研究報告</em>, vol. 2021–MUS–132, no. 22, Sep. 2021.</li>
<li>渡辺 瑠伊, 北村 大地, <ins>中村 友彦</ins>, 猿渡 洋, 高橋 祐,  近藤 多伸, “<strong>深層学習に基づく間引きインジケータ付き周波数帯域補間手法による音源分離処理の高速化</strong>,” <em>日本音響学会 2021年秋期研究発表会講演論文集</em>, pp. 155–158, Sep. 2021.</li>
<li>齋藤 弘一, <ins>中村 友彦</ins>, 矢田部 浩平,  猿渡 洋, “<strong>サンプリング周波数非依存音源分離モデルを用いた楽音分離の実験的評価</strong>,” <em>日本音響学会 2021年秋期研究発表会講演論文集</em>, pp. 159–162, Sep. 2021.</li>
<li>蓮実 拓也, <ins>中村 友彦</ins>, 高宗 典玄, 猿渡 洋, 北村 大地, 高橋 祐,  近藤 多伸, “<strong>Product of priors型確率分布を導入した音源モデルに基づく独立深層学習行列分析による多チャネル音源分離</strong>,” <em>日本音響学会 2021年秋期研究発表会講演論文集</em>, pp. 163–166, Sep. 2021.</li>
<li>三澤 颯大, <ins>中村 友彦</ins>, 高宗 典玄, 北村 大地,  猿渡 洋, “<strong>独立深層学習行列分析を用いたランク制約付き空間共分散行列推定による音声強調</strong>,” <em>日本音響学会 2021年秋期研究発表会講演論文集</em>, pp. 279–280, Sep. 2021.</li>
<li>成澤 直輝, 池下 林太郎, 高宗 典玄, 北村 大地, <ins>中村 友彦</ins>, 猿渡 洋,  中谷 智広, “<strong>独立深層学習行列分析を用いたランク制約付き空間共分散行列推定による音声強調</strong>,” <em>日本音響学会 2021年秋期研究発表会講演論文集</em>, pp. 301–304, Sep. 2021.</li>
<li><ins>中村 友彦</ins>,  猿渡 洋, “<strong>多重解像度深層分析を用いた楽音分離の実験的評価</strong>,” <em>情報処理学会研究報告</em>, vol. 2021–MUS–131, no. 13, pp. 1–11, Jun. 2021. <a href="https://drive.google.com/file/d/1wKeRtR1csgcaAwiyTZ5kYwOWBFydbhk6/view">slides<span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <span style="color: var(--md-code-hl-function-color)">[2021 Otogaku Symposium Best Presentation Award / 2021年度 音学シンポジウム優秀発表賞]</span></li>
<li>齋藤 弘一, <ins>中村 友彦</ins>, 矢田部 浩平,  猿渡 洋, “<strong>周波数領域でのフィルタ設計に基づくサンプリング周波数非依存畳み込み層を用いたDNN音源分離</strong>,” <em>情報処理学会研究報告</em>, vol. 2021–MUS–131, no. 21, pp. 1–6, Jun. 2021. <span style="color: var(--md-code-hl-function-color)">[2021 Otogaku Symposium Best Student Presentation Award (Awardee: Koichi Saito) / 2021年度 音学シンポジウム学生優秀発表賞（受賞者：齋藤 弘一）]</span></li>
<li>蓮実 拓也, <ins>中村 友彦</ins>, 高宗 典玄, 猿渡 洋, 北村 大地, 高橋 祐,  近藤 多伸, “<strong>非負値行列因子分解を導入したproduct of experts型音源モデルに基づく独立深層学習行列分析による多チャネル音源分離</strong>,” <em>情報処理学会研究報告</em>, vol. 2021–MUS–131, no. 37, pp. 1–8, Jun. 2021.</li>
<li>齋藤 弘一, <ins>中村 友彦</ins>, 矢田部 浩平, 小泉 悠馬,  猿渡 洋, “<strong>アンチエイリアシング機構を導入したサンプリング周波数非依存畳み込み層を用いた音源分離</strong>,” <em>情報処理学会研究報告</em>, vol. 2021–MUS–130, pp. 1–6, Mar. 2021.</li>
<li>齋藤 弘一, <ins>中村 友彦</ins>, 矢田部 浩平, 小泉 悠馬,  猿渡 洋, “<strong>潜在アナログフィルタ表現に基づく畳み込み層を用いたサンプリング周波数非依存なDNN音源分離</strong>,” <em>日本音響学会 2021年春季研究発表会講演論文集</em>, pp. 125–128, Mar. 2021.</li>
<li>蓮実 拓也, <ins>中村 友彦</ins>, 高宗 典玄, 猿渡 洋, 北村 大地, 高橋 祐,  近藤 多伸, “<strong>経験ベイズ独立深層学習行列分析による多チャネル音源分離</strong>,” <em>日本音響学会 2021年春季研究発表会講演論文集</em>, pp. 217–220, Mar. 2021. <span style="color: var(--md-code-hl-function-color)">[Best Student Presentation Award from ASJ（Awardee: Takuya Hasumi） / 日本音響学会 第22回学生優秀発表賞（受賞者：蓮実 拓也）]</span></li>
<li>成澤 直輝, 池下 林太郎, 高宗 典玄, 北村 大地, <ins>中村 友彦</ins>, 猿渡 洋,  中谷 智広, “<strong>独立深層学習テンソル分析に基づく多チャネル音源分離</strong>,” <em>日本音響学会 2021年春季研究発表会講演論文集</em>, pp. 117–120, Mar. 2021.</li>
<li>成澤 直輝, 高宗 典玄, 北村 大地, <ins>中村 友彦</ins>,  猿渡 洋, “<strong>音源分離のための周波数間相関を考慮した多変量複素Gauss分布に基づく深層学習による分散共分散行列推定の検討</strong>,” <em>日本音響学会 2020年秋季研究発表会講演論文集</em>, pp. 315–318, Sep. 2020.</li>
<li>小塚 詩穂里, <ins>中村 友彦</ins>,  猿渡 洋, “<strong>ニューラルネットワークとウェーブレット基底関数の同時学習に基づく多重解像度深層分析を用いた時間領域音源分離</strong>,” <em>電子情報通信学会技術研究報告</em>, vol. 119, no. 439, pp. 279–284, Mar. 2020.</li>
<li>高道 慎之介, 齋藤 佑樹, <ins>中村 友彦</ins>, 郡山 知樹,  猿渡 洋, “<strong>manga2voice: マンガ画像からの音声合成に向けた音声分析</strong>,” <em>日本音響学会2020年春季研究発表会講演論文集</em>, pp. 1065–1068, Mar. 2020.</li>
<li>小塚 詩穂里, <ins>中村 友彦</ins>,  猿渡 洋, “<strong>リフティングスキームによる離散ウェーブレット変換を導入した深層ニューラルネットに基づく時間領域音源分離</strong>,” <em>日本音響学会2020年春季研究発表会講演論文集</em>, pp. 325–328, Mar. 2020.</li>
<li><ins>中村 友彦</ins>,  猿渡 洋, “<strong>Haar変換を導入した時間領域深層ニューラルネットに基づく音源分離</strong>,” <em>電子情報通信学会技術研究報告</em>, vol. 119, no. 306, EA2019–60, pp. 41–48, Nov. 2019.</li>
<li><ins>中村 友彦</ins>,  亀岡 弘和, “<strong>高速近似連続ウェーブレット変換による振幅スペクトログラムからの逐次位相推定法</strong>,” <em>情報処理学会研究報告</em>, vol. 2016–MUS–111, no. 47, pp. 1–5, May 2016.</li>
<li><ins>中村 友彦</ins>,  亀岡 弘和, “<strong>非負値行列因子分解に基づく欠損データ補間による超解像声道スペクトル推定法</strong>,” <em>電子情報通信学会技術研究報告</em>, vol. 115, no. 523, SP2015–111, pp. 99–104, Mar. 2016.</li>
<li><ins>中村 友彦</ins>,  亀岡 弘和, “<strong>高速近似連続ウェーブレット変換による振幅スペクトログラムに対する実時間位相推定法</strong>,” <em>日本音響学会2016年春季研究発表会</em>, pp. 933–936, Mar. 2016.</li>
<li><ins>中村 友彦</ins>,  亀岡 弘和, “<strong>非負値行列因子分解に基づく欠損データ補間による声道スペクトル推定法の検討</strong>,” <em>日本音響学会2016年春季研究発表会</em>, pp. 393–396, Mar. 2016.</li>
<li><ins>中村 友彦</ins>,  亀岡 弘和, “<strong>全極スペクトルモデルと擬似周期信号モデルのウェーブレット変換表現を用いた多重音スペクトログラムの調波時間因子分解</strong>,” <em>情報処理学会研究報告</em>, vol. 2015–MUS–107, no. 50, pp. 1–8, May 2015. <span style="color: var(--md-code-hl-function-color)">[2015 Otogaku Symposium Award / 2015年度 音学シンポジウム優秀賞]</span></li>
<li><ins>中村 友彦</ins>,  亀岡 弘和, “<strong>全極スペクトルモデルを用いた調波時間因子分解による多重音解析</strong>,” <em>情報処理学会研究報告</em>, vol. 2015–MUS–106, no. 26, pp. 1–7, Mar. 2015.</li>
<li><ins>中村 友彦</ins>, 吉井 和佳, 後藤 真孝,  亀岡 弘和, “<strong>音楽音響信号中の調波音の周波数特性およびドラムの音色の置換システム</strong>,” <em>情報処理学会研究報告</em>, vol. 2014–MUS–104, no. 11, pp. 1–6, Aug. 2014.</li>
<li><ins>中村 友彦</ins>,  亀岡 弘和, “<strong>無矛盾性規準に基づく連続ウェーブレット変換スペクトログラムへの位相推定法と高速化</strong>,” <em>情報処理学会研究報告</em>, vol. 2014–MUS–103, no. 41, pp. 1–6, May 2014. <span style="color: var(--md-code-hl-function-color)">[IPSJ Yamashita SIG Research Award / 情報処理学会 2014年度山下記念研究賞]</span></li>
<li>樋口 卓也, 高宗 典玄, <ins>中村 友彦</ins>,  亀岡 弘和, “<strong>確率的モデル化に基づく移動音源の劣決定ブラインド音源分離</strong>,” <em>電子情報通信学会技術研究報告</em>, vol. 114, no. 52, pp. 211–216, May 2014.</li>
<li>四方 紘太郎, 高宗 典玄, <ins>中村 友彦</ins>,  亀岡 弘和, “<strong>調波時間因子分解法に基づく事前情報付き多重音解析</strong>,” <em>情報処理学会研究報告</em>, vol. 2014–MUS–103, no. 18, pp. 1–6, May 2014.</li>
<li><ins>中村 友彦</ins>,  亀岡 弘和, “<strong>連続ウェーブレット変換の高速近似アルゴリズムに基づく振幅スケーログラムへの無矛盾位相付加法の検討</strong>,” <em>日本音響学会2014年春季研究発表会講演集</em>, pp. 933–936, Mar. 2014.</li>
<li><ins>中村 友彦</ins>, 吉井 和佳, 後藤 真孝,  亀岡 弘和, “<strong>音楽音響信号に含まれる調波音の周波数特性とドラムの音色の転写システム</strong>,” <em>日本音響学会2014年春季研究発表会講演集</em>, pp. 1043–1044, Mar. 2014. <span style="color: var(--md-code-hl-function-color)">[Best Student Presentation Award from ASJ / 日本音響学会 第9回学生優秀発表賞]</span></li>
<li>四方 紘太郎, 高宗 典玄, <ins>中村 友彦</ins>,  亀岡 弘和, “<strong>調波時間因子分解に基づく音楽事前情報付き多重音解析</strong>,” <em>日本音響学会2014年春季研究発表会講演集</em>, pp. 1049–1052, Mar. 2014.</li>
<li>樋口 卓哉, 高宗 典玄, <ins>中村 友彦</ins>,  亀岡 弘和, “<strong>DOA-HMMに基づく劣決定ブラインド音源分離</strong>,” <em>日本音響学会2013年秋季研究発表会講演集</em>, pp. 23–26, Sep. 2013.</li>
<li><ins>中村 友彦</ins>, 中村 栄太,  嵯峨山 茂樹, “<strong>誤り・任意の弾き直し・弾き飛ばしを含む演奏音響信号への高速な楽譜追跡</strong>,” <em>情報処理学会研究報告</em>, vol. 2013–MUS–99, no. 40, pp. 1–5, May 2013.</li>
<li><ins>中村 友彦</ins>, 中村 栄太,  嵯峨山 茂樹, “<strong>弾き直し・弾き飛ばしを含む音楽演奏への高速な音響入力楽譜追跡</strong>,” <em>情報処理学会 第75回全国大会</em>, vol. 2013, no. 1, pp. 283–284, Mar. 2013. <span style="color: var(--md-code-hl-function-color)">[Student Encouragement Award of IPSJ National Convention / 情報処理学会第75回全国大会 学生奨励賞]</span></li>
<li><ins>中村 友彦</ins>, 水野 優, 鈴木 孝輔, 中村 栄太, 樋口 祐介, 深山 覚,  嵯峨山 茂樹, “<strong>音楽演奏の誤りや反復に頑健な音響入力自動伴奏</strong>,” <em>日本音響学会2012年秋季研究発表会講演集</em>, pp. 931–934, Sep. 2012.</li>
<li>中野 允裕, ルルー ジョナトン, 亀岡 弘和, <ins>中村 友彦</ins>, 小野 順貴,  嵯峨山 茂樹, “<strong>スペクトログラムのベイジアンノンパラメトリックモデリングに基づく音楽信号の解析</strong>,” <em>情報処理学会研究報告</em>, vol. 2011–MUS–91, no. 6, pp. 1–8, Jul. 2011.</li>
</ol>
<h2>Review Papers / 解説記事</h2>
<ol>
<li>亀岡 弘和, <ins>中村 友彦</ins>,  高宗 典玄, “<strong>音楽音響信号処理技術の最先端</strong>,” <em>電子情報通信学会誌</em>, vol. 98, no. 6, pp. 467–474, Jun. 2015.</li>
</ol>
<h2>Thesis / 学位論文</h2>
<!-- 1. Ph.D. Thesis: ^^Tomohiko Nakamura^^, **"Source-Filter Representation and Phase Estimation in Continuous Wavelet Transform Domain for Monaural Music Audio Editing,"** the University of Tokyo, Mar. 2016.   
   <span style="color: var(--md-code-hl-function-color)">[Dean’s Award, Graduate School of Information Science and Technology, The University of Tokyo / 東京大学大学院情報理工学系研究科 研究科長賞], [IPSJ SIG-MUS Recommended Ph.D. Thesis / 情報処理学会 2015年度研究会推薦博士論文]</span> -->
<ol>
<li>Ph.D Thesis: <ins>Tomohiko Nakamura</ins>, “<strong>Source-Filter Representation and Phase Estimation in Continuous Wavelet Transform Domain for Monaural Music Audio Editing</strong>,” PhD thesis, The University of Tokyo, Mar. 2016. <a href="https://doi.org/10.15083/00073992"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <a href="https://repository.dl.itc.u-tokyo.ac.jp/records/48867#.YK0RbKj7QuU"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M369.9 97.9 286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"/></svg></span></a>, <span style="color: var(--md-code-hl-function-color)">[Dean’s Award, Graduate School of Information Science and Technology, The University of Tokyo / 東京大学大学院情報理工学系研究科 研究科長賞, IPSJ SIG-MUS Recommended Ph.D. Thesis / 情報処理学会 2015年度研究会推薦博士論文]</span></li>
<li>Master Thesis: <ins>Tomohiko Nakamura</ins>, <strong>"Fast Score Following to Acoustic Signal of Musical Performance with Errors, Repeats and Skips,"</strong> the University of Tokyo, Mar. 2013. (in Japanese)
   <span style="color: var(--md-code-hl-function-color)">[IPSJ Certificate of Excellent Master’s Thesis / 情報処理学会第75回全国大会 情報処理学会推奨 修士論文認定]</span></li>
<li>Bachelor Thesis: <ins>Tomohiko Nakamura</ins>, <strong>"Local Stability Analysis of Cell-to-Cell Networks with Cyclic Gene Regulatory Networks,"</strong> the University of Tokyo, Mar. 2011. (in Japanese)</li>
</ol>
<h2>Patents / 特許</h2>
<ol>
<li><ins>中村 友彦</ins>, “<strong>対象物認識装置、対象物認識方法、及び対象物認識プログラム</strong>,” 特開2021-033374, 01-Mar-2021.</li>
<li><ins>中村 友彦</ins>, “<strong>学習済みモデル、学習装置、学習方法、及び学習プログラム</strong>,” 特開2021-033395, 01-Mar-2021.</li>
<li><ins>中村 友彦</ins>, “<strong>対象物認識装置、対象物認識方法、及び対象物認識プログラム</strong>,” 特開2021-026685, 22-Feb-2021.</li>
<li><ins>中村 友彦</ins>, 國松 昇平, 櫻井 利彦,  大西 一徳, “<strong>カメラ配置評価装置、カメラ配置評価方法、及びコンピュータプログラム</strong>,” 特開2021-10070, 28-Jan-2021.</li>
<li><ins>中村 友彦</ins>, “<strong>対象物認識装置、対象物認識方法、及び対象物認識プログラム</strong>,” 特許第6773829号, 21-Oct-2020.</li>
<li><ins>中村 友彦</ins>, “<strong>学習装置、学習方法、学習プログラム、及び対象物認識装置</strong>,” 特許第6773825号, 21-Oct-2020.</li>
<li><ins>中村 友彦</ins>, “<strong>データベース統合装置、データベース統合方法、データベース統合プログラム、及びデータ補完装置</strong>,” 特許第6768101号, 14-Oct-2020.</li>
<li><ins>中村 友彦</ins>, 伊藤 忠彦,  島岡 政基, “<strong>証明書管理装置</strong>,” 特許第6647259号, 16-Jan-2020.</li>
<li><ins>中村 友彦</ins>,  亀岡 弘和, “<strong>声道スペクトル推定装置、声道スペクトル推定方法、及びプログラム</strong>,” 特許第6420781号, 07-Nov-2018.</li>
</ol>
<h2>Awards / 受賞</h2>
<h3>Awards of My Papers / 自身の受賞</h3>
<ol>
<li>2021/07: 2021 Encouragement Award, Foundation of the Promotion of Engineering Research / 総合研究奨励会 令和2年度総合研究奨励賞</li>
<li>2021/06: 2021 Otogaku Symposium Best Presentation Award / 2021年度 音学シンポジウム優秀発表賞</li>
<li>2016/08: IPSJ SIG-MUS Recommended Ph.D. Thesis / 情報処理学会 2015年度研究会推薦博士論文</li>
<li>2016/03: Dean’s Award from Graduate School of Information Science and Technology, The University of Tokyo / 東京大学 大学院情報理工学系研究科 研究科長賞</li>
<li>2016/03: IPSJ Yamashita SIG Research Award / 情報処理学会 2015年度山下記念研究賞</li>
<li>2015/10: SICE Best Paper Award (Takeda Award) / 計測自動制御学会 論文賞（武田賞）</li>
<li>2015/05: 2015 Otogaku Symposium Award / 2015年度 音学シンポジウム優秀賞</li>
<li>2014/03: Best Student Presentation Award from ASJ / 日本音響学会 第9回学生優秀発表賞</li>
<li>2013/03: IPSJ Certificate of Excellent Master’s Thesis / 情報処理学会第75回全国大会 情報処理学会推奨 修士論文認定</li>
<li>2013/03: Student Encouragement Award of IPSJ National Convention / 情報処理学会第75回全国大会 学生奨励賞</li>
<li>2011/09: SICE Annual Conference 2011 International Award</li>
<li>2011/09: SICE Annual Conference 2011 Finalist of Young Author Award</li>
</ol>
<h3>Awards Received by My Students / 指導学生の受賞</h3>
<ol>
<li>2021/06: 2021 Otogaku Symposium Best Student Presentation Award (Awardee: Koichi Saito) / 2021年度 音学シンポジウム学生優秀発表賞（受賞者：齋藤 弘一）</li>
<li>2021/05: Best Student Presentation Award from ASJ (Awardee: Takuya Hasumi) / 日本音響学会 第22回学生優秀発表賞（受賞者：蓮実 拓也）</li>
</ol>
<h2>Other / その他</h2>
<ol>
<li><ins>中村 友彦</ins>, 吉井 和佳, 後藤 真孝, 亀岡 弘和, <strong>"音楽音響信号中の調波音の周波数特性およびドラムの音色の置換システム,"</strong> <em>OngaCRESTシンポジウム2014-音楽情報処理研究が切り拓く未来を探る-,</em> Aug. 23, 2014.<ul>
<li><a href="http://ongacrest.jp/">OngaCREST</a></li>
<li><a href="http://av.watch.impress.co.jp/docs/series/dal/20140825_663420.html">「人型ロボットのダンス」や「声の年齢制御」など、音楽情報処理の最先端をレポート - 藤本健のDigital Audio Laboratory</a></li>
<li><a href="http://pc.watch.impress.co.jp/docs/column/kyokai/20140827_663740.html">コピー不可能な体験を価値の中核に～音楽情報処理の「OngaCRESTシンポジウム」レポート - 森山和道の「ヒトと機械の境界面」</a></li>
</ul>
</li>
</ol>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="research.html" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Research / Demo
            </div>
          </div>
        </a>
      
      
        <a href="lecture.html" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Lecture Notes
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2021- Tomohoiko Nakamura
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": ".", "features": ["navigation.tabs", "navigation.tabs.sticky", "toc.integrate"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "assets/javascripts/workers/search.d351de03.min.js", "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.a1609d9a.min.js"></script>
      
    
  </body>
</html>