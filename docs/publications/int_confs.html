
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Tomohiko Nakamura">
      
      
        <link rel="canonical" href="https://tomohikonakamura.github.io/Tomohiko-Nakamura/publications/int_confs.html">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.33">
    
    
      
        <title>International Conferences & Workshops / 国際会議 - Tomohiko Nakamura</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9EPRVMYY0Z"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9EPRVMYY0Z');
</script>
<script>
  function toggleBib(id) {
    var elem = document.getElementById(id);
    if (elem.style.display === "none") {
      elem.style.display = "block";
    } else {
      elem.style.display = "none";
    }
  }
</script>
    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="grey" data-md-color-accent="primary">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#international-conferences-workshops" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="Tomohiko Nakamura" class="md-header__button md-logo" aria-label="Tomohiko Nakamura" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Tomohiko Nakamura
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              International Conferences & Workshops / 国際会議
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="Tomohiko Nakamura" class="md-nav__button md-logo" aria-label="Tomohiko Nakamura" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Tomohiko Nakamura
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../research.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Research / Demo
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Publications
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../datasets.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Datasets
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lecture Notes
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../tips.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tips
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="international-conferences-workshops">International Conferences &amp; Workshops / 国際会議<a class="headerlink" href="#international-conferences-workshops" title="Permanent link">&para;</a></h1>
<ol>
<li>Yuto Ishikawa, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, and Hiroshi Saruwatari, “<strong>Hearing-aids system using distributed assistive device and blind speech extraction method under diffuse noise</strong>,” in <em>International Congress on Acoustics</em>, May 2025.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('YIshikawa202505ICA')">bib</a><br><div id="YIshikawa202505ICA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">YIshikawa202505ICA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Ishikawa, Yuto and Nakamura, Tomohiko and Takamune, Norihiro and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;International Congress on Acoustics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Hearing-Aids System Using Distributed Assistive Device and Blind Speech Extraction Method under Diffuse Noise&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2025&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;May&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins>, Kwanghee Choi, Keigo Hojo, Yoshiaki Bando, Satoru Fukayama, and Shinji Watanabe, “<strong>Discrete speech unit extraction via independent component analysis</strong>,” in <em>SALMA: Speech and Audio Language Models - Architectures, Data Sources, and Training Paradigms, IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops</em>, Apr. 2025.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura2025ICASSPWSALMA')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2501.06562">arXiv</a>   <a class="md-button md-button--small" href="https://drive.google.com/file/d/11dGm8IPG98nJVbRwiOQJzgSI3OCI_zaJ/view?usp=sharing">poster <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a>   <a class="md-button md-button--small" href="https://github.com/TomohikoNakamura/ica_dsu_espnet">code <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a><br><div id="TNakamura2025ICASSPWSALMA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TNakamura2025ICASSPWSALMA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Choi, Kwanghee and Hojo, Keigo and Bando, Yoshiaki and Fukayama, Satoru and Watanabe, Shinji&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;SALMA: Speech and Audio Language Models - Architectures, Data Sources, and Training Paradigms, {IEEE} International Conference on Acoustics, Speech, and Signal Processing Workshops&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Discrete Speech Unit Extraction via Independent Component Analysis&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2025&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;April&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Yuto Ishikawa, Osamu Take, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.1109/APSIPAASC63619.2025.10848599">Real-time noise estimation for Lombard-effect speech synthesis in human–avatar dialogue systems</a></strong>,” in <em>Asia Pacific Signal and Information Processing Association Annual Summit and Conference</em>, Dec. 2024.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('YIshikawa202412APSIPA')">bib</a>   <a class="md-button md-button--small" href="http://www.apsipa2024.org/files/papers/355.pdf">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a><br><div id="YIshikawa202412APSIPA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">YIshikawa202412APSIPA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Ishikawa, Yuto and Take, Osamu and Nakamura, Tomohiko and Takamune, Norihiro and Saito, Yuki and Takamichi, Shinnosuke and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Real-Time Noise Estimation for {Lombard}-Effect Speech Synthesis in Human--Avatar Dialogue Systems&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Asia Pacific Signal and Information Processing Association Annual Summit and Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;December&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/APSIPAASC63619.2025.10848599&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Hiroaki Hyodo, Shinnosuke Takamichi, <ins>Tomohiko Nakamura</ins>, Junya Koguchi, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.1109/SLT61566.2024.10832340">DNN-based ensemble singing voice synthesis with interactions between singers</a></strong>,” in <em>IEEE Spoken Language Technology Workshop</em>, Dec. 2024, pp. 660–667.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('HHyodo202412SLT')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2409.09988">arXiv</a>   <a class="md-button md-button--small" href="https://github.com/sarulab-speech/ensemble_svs_with_interactions">code <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a><br><div id="HHyodo202412SLT" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">HHyodo202412SLT</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Hyodo, Hiroaki and Takamichi, Shinnosuke and Nakamura, Tomohiko and Koguchi, Junya and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;IEEE Spoken Language Technology Workshop&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;DNN-based ensemble singing voice synthesis with interactions between singers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;December&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;660--667&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/SLT61566.2024.10832340&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Hitoshi Suda, Shunsuke Yoshida, <ins>Tomohiko Nakamura</ins>, Fukayama Satoru, and Jun Ogata, “<strong><a href="https://doi.org/10.5281/ZENODO.14877287">FruitsMusic: A real-world corpus of Japanese idol-group songs</a></strong>,” in <em>International Society for Music Information Retrieval Conference</em>, Nov. 2024.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('HSuda202411ISMIR')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2409.12549">arXiv</a>   <a class="md-button md-button--small" href="https://huggingface.co/datasets/fruits-music/fruits-music">dataset <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0s224 35.8 224 80zm-54.8 134.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432v-85.9z"/></svg></span></a><br><div id="HSuda202411ISMIR" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">HSuda202411ISMIR</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Suda, Hitoshi and Yoshida, Shunsuke and Nakamura, Tomohiko and Satoru, Fukayama and Ogata, Jun&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;International Society for Music Information Retrieval Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{FruitsMusic: A} Real-World Corpus of {Japanese} Idol-Group Songs&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.5281/ZENODO.14877287&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Yuta Amezawa, <ins>Tomohiko Nakamura</ins>, Satoru Fukayama, Takahiro Shiina, and Takahiko Uchide, “<strong>Automatic extraction and peak arrival estimation of later phase in S coda</strong>,” in <em>International Joint Workshop on Slow-to-Fast Earthquakes 2024</em>, Sep. 2024.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('YAmezawa202409IJWStFE')">bib</a><br><div id="YAmezawa202409IJWStFE" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">YAmezawa202409IJWStFE</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Amezawa, Yuta and Nakamura, Tomohiko and Fukayama, Satoru and Shiina, Takahiro and Uchide, Takahiko&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;International Joint Workshop on Slow-to-Fast Earthquakes 2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Automatic extraction and peak arrival estimation of later phase in {S} coda&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Yuto Ishikawa, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, and Hiroshi Saruwatari, “<strong>Real-time framework for speech extraction based on independent low-rank matrix analysis with spatial regularization and rank-constrained spatial covariance matrix estimation</strong>,” in <em>Workshop on Spoken Dialogue Systems for Cybernetic Avatars (SDS4CA)</em>, Sep. 2024.<br />
 (Presentation only)<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('YIshikawa202409SDS4CA')">bib</a><br><div id="YIshikawa202409SDS4CA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">YIshikawa202409SDS4CA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Ishikawa, Yuto and Nakamura, Tomohiko and Takamune, Norihiro and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Real-Time Framework for Speech Extraction Based on Independent Low-Rank Matrix Analysis with Spatial Regularization and Rank-Constrained Spatial Covariance Matrix Estimation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Workshop on Spoken Dialogue Systems for Cybernetic Avatars (SDS4CA)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">note</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;(Presentation only)&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Kwanghee Choi, Ankita Pasad, <ins>Tomohiko Nakamura</ins>, Satoru Fukayama, Karen Livescu, and Shinji Watanabe, “<strong><a href="https://doi.org/10.21437/Interspeech.2024-1157">Self-supervised speech representations are more phonetic than semantic</a></strong>,” in <em>INTERSPEECH</em>, Sep. 2024, pp. 4578–4582.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('KChoi202409INTERSPEECH')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2406.08619">arXiv</a>   <a class="md-button md-button--small" href="https://github.com/juice500ml/phonetic_semantic_probing">code <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a><br><div id="KChoi202409INTERSPEECH" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KChoi202409INTERSPEECH</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Choi, Kwanghee and Pasad, Ankita and Nakamura, Tomohiko and Fukayama, Satoru and Livescu, Karen and Watanabe, Shinji&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;INTERSPEECH&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Self-Supervised Speech Representations are More Phonetic than Semantic&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;4578--4582&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.21437/Interspeech.2024-1157&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Yoshiaki Bando, <ins>Tomohiko Nakamura</ins>, and Shinji Watanabe, “<strong><a href="https://doi.org/10.21437/Interspeech.2024-1137">Neural blind source separation and diarization for distant speech recognition</a></strong>,” in <em>INTERSPEECH</em>, Sep. 2024, pp. 722–726.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('YBando202409INTERSPEECH')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2406.08396">arXiv</a>   <a class="md-button md-button--small" href="https://ybando.jp/projects/neural-fcasa/">demo <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M499.1 6.3c8.1 6 12.9 15.6 12.9 25.7v336c0 44.2-43 80-96 80s-96-35.8-96-80 43-80 96-80c11.2 0 22 1.6 32 4.6V147l-256 76.8V432c0 44.2-43 80-96 80S0 476.2 0 432s43-80 96-80c11.2 0 22 1.6 32 4.6V128c0-14.1 9.3-26.6 22.8-30.7l320-96c9.7-2.9 20.2-1.1 28.3 5z"/></svg></span></a>   <a class="md-button md-button--small" href="https://github.com/b-sigpro/neural-fcasa">code <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a><br><div id="YBando202409INTERSPEECH" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">YBando202409INTERSPEECH</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Bando, Yoshiaki and Nakamura, Tomohiko and Watanabe, Shinji&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;INTERSPEECH&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Neural Blind Source Separation and Diarization for Distant Speech Recognition&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;722--726&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.21437/Interspeech.2024-1137&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Yuto Ishikawa, Kohei Konaka, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.1109/ICASSPW62465.2024.10627448">Real-time speech extraction using spatially regularized independent low-rank matrix analysis and rank-constrained spatial covariance matrix estimation</a></strong>,” in <em>Hands-Free Speech Communication and Microphone Arrays, IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops</em>, Apr. 2024, pp. 730–734.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('YIshikawa202404ICASSPWHSCMA')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2403.12477v1">arXiv</a><br><div id="YIshikawa202404ICASSPWHSCMA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">YIshikawa202404ICASSPWHSCMA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Ishikawa, Yuto and Konaka, Kohei and Nakamura, Tomohiko and Takamune, Norihiro and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Real-time Speech Extraction Using Spatially Regularized Independent Low-rank Matrix Analysis and Rank-Constrained Spatial Covariance Matrix Estimation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Hands-free Speech Communication and Microphone Arrays, IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;April&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;730--734&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/ICASSPW62465.2024.10627448&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Kanami Imamura, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, Kohei Yatabe, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.23919/EUSIPCO58844.2023.10289819">Algorithms of sampling-frequency-independent layers for non-integer strides</a></strong>,” in <em>European Signal Processing Conference</em>, Sep. 2023, pp. 326–330.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('KImamura202309EUSIPCO')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2306.10718">arXiv</a><br><div id="KImamura202309EUSIPCO" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KImamura202309EUSIPCO</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Imamura, Kanami and Nakamura, Tomohiko and Takamune, Norihiro and Yatabe, Kohei and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Algorithms of Sampling-Frequency-Independent Layers for Non-integer Strides&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;European Signal Processing Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2023&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;326--330&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.23919/EUSIPCO58844.2023.10289819&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Joonyong Park, Shinnosuke Takamichi, <ins>Tomohiko Nakamura</ins>, Kentaro Seki, Detai Xin, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.21437/Interspeech.2023-981">How generative spoken language model encodes noisy speech: Investigation from phonetics to syntactics</a></strong>,” in <em>INTERSPEECH</em>, Aug. 2023, pp. 1085–1089.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('JPark202308INTERSPEECH')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2306.00697">arXiv</a><br><div id="JPark202308INTERSPEECH" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">JPark202308INTERSPEECH</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Park, Joonyong and Takamichi, Shinnosuke and Nakamura, Tomohiko and Seki, Kentaro and Xin, Detai and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;How Generative Spoken Language Model Encodes Noisy Speech{: I}nvestigation from Phonetics to Syntactics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;INTERSPEECH&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;1085--1089&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;August&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2023&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.21437/Interspeech.2023-981&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins>, Shinnosuke Takamichi, Naoko Tanji, Satoru Fukayama, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.1109/ICASSP49357.2023.10095569"><span class="nocase">jaCappella corpus: A japanese a cappella vocal ensemble corpus</a></strong>,” in <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, Jun. 2023.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura202306ICASSP')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2211.16028">arXiv</a>   <a class="md-button md-button--small" href="demo/jaCappella_sep">demo <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M499.1 6.3c8.1 6 12.9 15.6 12.9 25.7v336c0 44.2-43 80-96 80s-96-35.8-96-80 43-80 96-80c11.2 0 22 1.6 32 4.6V147l-256 76.8V432c0 44.2-43 80-96 80S0 476.2 0 432s43-80 96-80c11.2 0 22 1.6 32 4.6V128c0-14.1 9.3-26.6 22.8-30.7l320-96c9.7-2.9 20.2-1.1 28.3 5z"/></svg></span></a>   <a class="md-button md-button--small" href="https://github.com/TomohikoNakamura/asteroid_jaCappella/tree/jaCappella/egs/jaCappella">code <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a>   <a class="md-button md-button--small" href="https://tomohikonakamura.github.io/jaCappella_corpus/">dataset <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0s224 35.8 224 80zm-54.8 134.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432v-85.9z"/></svg></span></a><br><div id="TNakamura202306ICASSP" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TNakamura202306ICASSP</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Takamichi, Shinnosuke and Tanji, Naoko and Fukayama, Satoru and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{jaCappella corpus: A} Japanese a cappella vocal ensemble corpus&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;IEEE International Conference on Acoustics, Speech, and Signal Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;June&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2023&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/ICASSP49357.2023.10095569&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Kota Arai, Yutaro Hirao, Takuji Narumi, <ins>Tomohiko Nakamura</ins>, Shinnosuke Takamichi, and Shigeo Yoshida, “<strong><a href="https://doi.org/10.1145/3581641.3584053">TimToShape: Supporting practice of musical instruments by visualizing timbre with 2D shapes based on crossmodal correspondences</a></strong>,” in <em>ACM Conference on Intelligent User Interfaces</em>, Mar. 2023, pp. 850–865.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('KArai202303ACMIUI')">bib</a>   <a class="md-button md-button--small" href="https://medium.com/@shigeo.yoshida/musical-instrument-practice-support-system-based-on-crossmodal-correspondences-to-be-presented-at-500a02fd22b0">blog <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M192 32c0 17.7 14.3 32 32 32 123.7 0 224 100.3 224 224 0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32zm0 96c0 17.7 14.3 32 32 32 70.7 0 128 57.3 128 128 0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192-17.7 0-32 14.3-32 32zm-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48-48-21.5-48-48V144z"/></svg></span></a><br><div id="KArai202303ACMIUI" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KArai202303ACMIUI</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Arai, Kota and Hirao, Yutaro and Narumi, Takuji and Nakamura, Tomohiko and Takamichi, Shinnosuke and Yoshida, Shigeo&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{TimToShape: S}upporting Practice of Musical Instruments by Visualizing Timbre with 2D Shapes based on Crossmodal Correspondences&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;ACM Conference on Intelligent User Interfaces&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;March&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2023&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;850--865&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1145/3581641.3584053&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Futa Nakashima, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, Satoru Fukayama, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.23919/APSIPAASC55919.2022.9980158">Hyperbolic timbre embedding for musical instrument sound synthesis based on variational autoencoders</a></strong>,” in <em>Asia Pacific Signal and Information Processing Association Annual Summit and Conference</em>, Nov. 2022, pp. 736–743.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('FNakashima202211APSIPA')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2209.13211">arXiv</a><br><div id="FNakashima202211APSIPA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">FNakashima202211APSIPA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakashima, Futa and Nakamura, Tomohiko and Takamune, Norihiro and Fukayama, Satoru and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Hyperbolic timbre embedding for musical instrument sound synthesis based on variational autoencoders&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Asia Pacific Signal and Information Processing Association Annual Summit and Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2022&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;736--743&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.23919/APSIPAASC55919.2022.9980158&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Yuki Ito, <ins>Tomohiko Nakamura</ins>, Shoichi Koyama, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.1109/IWAENC53105.2022.9914751">Head-related transfer function interpolation from spatially sparse measurements using autoencoder with source position conditioning</a></strong>,” in <em>International Workshop on Acoustic Signal Enhancement</em>, Sep. 2022.<br />
<span style="color: var(--md-code-hl-function-color)">[Finalist of Best Student Paper Award of IWAENC 2022 (Yuki Ito)]</span><br><a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('YIto2022IWAENC')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2207.10967">arXiv</a>   <a class="md-button md-button--small" href="https://github.com/ikets/HRTFInterpAE_public/blob/main/docs/Ito_IWAENC2022_public.pdf">slides <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a>   <a class="md-button md-button--small" href="https://ikets.github.io/HRTFInterpAE_public/">demo <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M499.1 6.3c8.1 6 12.9 15.6 12.9 25.7v336c0 44.2-43 80-96 80s-96-35.8-96-80 43-80 96-80c11.2 0 22 1.6 32 4.6V147l-256 76.8V432c0 44.2-43 80-96 80S0 476.2 0 432s43-80 96-80c11.2 0 22 1.6 32 4.6V128c0-14.1 9.3-26.6 22.8-30.7l320-96c9.7-2.9 20.2-1.1 28.3 5z"/></svg></span></a>   <a class="md-button md-button--small" href="https://github.com/ikets/HRTFInterpAE_public">code <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a><br><div id="YIto2022IWAENC" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">YIto2022IWAENC</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Ito, Yuki and Nakamura, Tomohiko and Koyama, Shoichi and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;International Workshop on Acoustic Signal Enhancement&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Head-related transfer function interpolation from spatially sparse measurements using autoencoder with source position conditioning&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2022&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/IWAENC53105.2022.9914751&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Kazuhide Shigemi, Shoichi Koyama, <ins>Tomohiko Nakamura</ins>, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.1109/IWAENC53105.2022.9914792">Physics-informed convolutional neural network with bicubic spline interpolation for sound field estimation</a></strong>,” in <em>International Workshop on Acoustic Signal Enhancement</em>, Sep. 2022.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('KShigemi2022IWAENC')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2207.10937">arXiv</a><br><div id="KShigemi2022IWAENC" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KShigemi2022IWAENC</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Shigemi, Kazuhide and Koyama, Shoichi and Nakamura, Tomohiko and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;International Workshop on Acoustic Signal Enhancement&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Physics-informed convolutional neural network with bicubic spline interpolation for sound field estimation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2022&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/IWAENC53105.2022.9914792&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Takaaki Saeki, Shinnosuke Takamichi, <ins>Tomohiko Nakamura</ins>, Naoko Tanji, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.21437/Interspeech.2022-298">SelfRemaster: Self-supervised speech restoration with analysis-by-synthesis approach using channel modeling</a></strong>,” in <em>INTERSPEECH</em>, Sep. 2022, pp. 4406–4410.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TSaeki202209INTERSPEECH')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2203.12937">arXiv</a>   <a class="md-button md-button--small" href="https://takaaki-saeki.github.io/ssl_remaster_demo/">demo <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M499.1 6.3c8.1 6 12.9 15.6 12.9 25.7v336c0 44.2-43 80-96 80s-96-35.8-96-80 43-80 96-80c11.2 0 22 1.6 32 4.6V147l-256 76.8V432c0 44.2-43 80-96 80S0 476.2 0 432s43-80 96-80c11.2 0 22 1.6 32 4.6V128c0-14.1 9.3-26.6 22.8-30.7l320-96c9.7-2.9 20.2-1.1 28.3 5z"/></svg></span></a>   <a class="md-button md-button--small" href="https://github.com/Takaaki-Saeki/ssl_speech_restoration">code <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a><br><div id="TSaeki202209INTERSPEECH" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TSaeki202209INTERSPEECH</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Saeki, Takaaki and Takamichi, Shinnosuke and Nakamura, Tomohiko and Tanji, Naoko and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;INTERSPEECH&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{SelfRemaster: S}elf-supervised speech restoration with analysis-by-synthesis approach using channel modeling&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;4406--4410&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2022&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.21437/Interspeech.2022-298&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Masaya Kawamura, <ins>Tomohiko Nakamura</ins>, Daichi Kitamura, Hiroshi Saruwatari, Yu Takahashi, and Kazunobu Kondo, “<strong><a href="https://doi.org/10.1109/ICASSP43922.2022.9746399">Differentiable digital signal processing mixture model for synthesis parameter extraction from mixture of harmonic sounds</a></strong>,” in <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, May 2022, pp. 941–945.<br />
<span style="color: var(--md-code-hl-function-color)">[IEEE Signal Processing Society Japan Student Conference Paper Award (Awardee: Masaya Kawamura) / 第16回 IEEE Signal Processing Society Japan Student Conference Paper Award（受賞者：川村 真也）]</span><br><a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('MKawamura202205ICASSP')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2202.00200">arXiv</a>   <a class="md-button md-button--small" href="https://sarulab-audio.github.io/DDSP_Mixture_Model/">demo <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M499.1 6.3c8.1 6 12.9 15.6 12.9 25.7v336c0 44.2-43 80-96 80s-96-35.8-96-80 43-80 96-80c11.2 0 22 1.6 32 4.6V147l-256 76.8V432c0 44.2-43 80-96 80S0 476.2 0 432s43-80 96-80c11.2 0 22 1.6 32 4.6V128c0-14.1 9.3-26.6 22.8-30.7l320-96c9.7-2.9 20.2-1.1 28.3 5z"/></svg></span></a><br><div id="MKawamura202205ICASSP" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">MKawamura202205ICASSP</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Kawamura, Masaya and Nakamura, Tomohiko and Kitamura, Daichi and Saruwatari, Hiroshi and Takahashi, Yu and Kondo, Kazunobu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Differentiable digital signal processing mixture model for synthesis parameter extraction from mixture of harmonic sounds&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;IEEE International Conference on Acoustics, Speech, and Signal Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2022&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;May&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;941--945&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/ICASSP43922.2022.9746399&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Takuya Hasumi, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, Hiroshi Saruwatari, Daichi Kitamura, Yu Takahashi, and Kazunobu Kondo, “<strong>Multichannel audio source separation with independent deeply learned matrix analysis using product of source models</strong>,” in <em>Asia Pacific Signal and Information Processing Association Annual Summit and Conference</em>, Dec. 2021, pp. 1226–1233.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('THasumi202112APSIPA')">bib</a>   <a class="md-button md-button--small" href="https://ieeexplore.ieee.org/document/9689636">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2109.00704">arXiv</a><br><div id="THasumi202112APSIPA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">THasumi202112APSIPA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Hasumi, Takuya and Nakamura, Tomohiko and Takamune, Norihiro and Saruwatari, Hiroshi and Kitamura, Daichi and Takahashi, Yu and Kondo, Kazunobu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Multichannel audio source separation with independent deeply learned matrix analysis using product of source models&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Asia Pacific Signal and Information Processing Association Annual Summit and Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2021&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;December&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;1226--1233&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Sota Misawa, Norihiro Takamune, <ins>Tomohiko Nakamura</ins>, Daichi Kitamura, Hiroshi Saruwatari, Masakazu Une, and Shoji Makino, “<strong>Speech enhancement by noise self-supervised rank-constrained spatial covariance matrix estimation via independent deeply learned matrix analysis</strong>,” in <em>Asia Pacific Signal and Information Processing Association Annual Summit and Conference</em>, Dec. 2021, pp. 578–584.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('SMisawa202112APSIPA')">bib</a>   <a class="md-button md-button--small" href="https://ieeexplore.ieee.org/document/9689665/">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2109.04658">arXiv</a><br><div id="SMisawa202112APSIPA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SMisawa202112APSIPA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Misawa, Sota and Takamune, Norihiro and Nakamura, Tomohiko and Kitamura, Daichi and Saruwatari, Hiroshi and Une, Masakazu and Makino, Shoji&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Speech enhancement by noise self-supervised rank-constrained spatial covariance matrix estimation via independent deeply learned matrix analysis&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Asia Pacific Signal and Information Processing Association Annual Summit and Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2021&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;December&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;578--584&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Yusaku Mizobuchi, Daichi Kitamura, <ins>Tomohiko Nakamura</ins>, Hiroshi Saruwatari, Yu Takahashi, and Kazunobu Kondo, “<strong>Prior distribution design for music bleeding-sound reduction based on nonnegative matrix factorization</strong>,” in <em>Asia Pacific Signal and Information Processing Association Annual Summit and Conference</em>, Dec. 2021, pp. 651–658.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('YMizobuchi202112APSIPA')">bib</a>   <a class="md-button md-button--small" href="https://ieeexplore.ieee.org/document/9689601/">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2109.00237">arXiv</a><br><div id="YMizobuchi202112APSIPA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">YMizobuchi202112APSIPA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Mizobuchi, Yusaku and Kitamura, Daichi and Nakamura, Tomohiko and Saruwatari, Hiroshi and Takahashi, Yu and Kondo, Kazunobu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Prior distribution design for music bleeding-sound reduction based on nonnegative matrix factorization&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Asia Pacific Signal and Information Processing Association Annual Summit and Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2021&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;December&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;651--658&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Koichi Saito, <ins>Tomohiko Nakamura</ins>, Kohei Yatabe, Yuma Koizumi, and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.23919/EUSIPCO54536.2021.9615941">Sampling-frequency-independent audio source separation using convolution layer based on impulse invariant method</a></strong>,” in <em>European Signal Processing Conference</em>, Aug. 2021, pp. 321–325.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('KSaito202108EUSIPCO')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2105.04079">arXiv</a><br><div id="KSaito202108EUSIPCO" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KSaito202108EUSIPCO</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Saito, Koichi and Nakamura, Tomohiko and Yatabe, Kohei and Koizumi, Yuma and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sampling-frequency-independent audio source separation using convolution layer based on impulse invariant method&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;European Signal Processing Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2021&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;August&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;321--325&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.23919/EUSIPCO54536.2021.9615941&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Naoki Narisawa, Rintaro Ikeshita, Norihiro Takamune, Daichi Kitamura, <ins>Tomohiko Nakamura</ins>, Hiroshi Saruwatari, and Tomohiro Nakatani, “<strong><a href="https://doi.org/10.23919/eusipco54536.2021.9616300">Independent deeply learned tensor analysis for determined audio source separation</a></strong>,” in <em>European Signal Processing Conference</em>, Aug. 2021, pp. 326–330.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('NNarisawa202108EUSIPCO')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2106.05529">arXiv</a><br><div id="NNarisawa202108EUSIPCO" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NNarisawa202108EUSIPCO</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Narisawa, Naoki and Ikeshita, Rintaro and Takamune, Norihiro and Kitamura, Daichi and Nakamura, Tomohiko and Saruwatari, Hiroshi and Nakatani, Tomohiro&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Independent deeply learned tensor analysis for determined audio source separation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;European Signal Processing Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2021&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;August&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;326--330&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.23919/eusipco54536.2021.9616300&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Takuya Hasumi, <ins>Tomohiko Nakamura</ins>, Norihiro Takamune, Hiroshi Saruwatari, Daichi Kitamura, Yu Takahashi, and Kazunobu Kondo, “<strong><a href="https://doi.org/10.23919/eusipco54536.2021.9616245">Empirical bayesian independent deeply learned matrix analysis for multichannel audio source separation</a></strong>,” in <em>European Signal Processing Conference</em>, Aug. 2021, pp. 331–335.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('THasumi202108EUSIPCO')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2106.03492">arXiv</a><br><div id="THasumi202108EUSIPCO" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">THasumi202108EUSIPCO</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Hasumi, Takuya and Nakamura, Tomohiko and Takamune, Norihiro and Saruwatari, Hiroshi and Kitamura, Daichi and Takahashi, Yu and Kondo, Kazunobu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Empirical bayesian independent deeply learned matrix analysis for multichannel audio source separation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;European Signal Processing Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2021&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;August&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;331--335&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.23919/eusipco54536.2021.9616245&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Shihori Kozuka, <ins>Tomohiko Nakamura</ins>, and Hiroshi Saruwatari, “<strong>Investigation on wavelet basis function of DNN-based time domain audio source separation inspired by multiresolution analysis</strong>,” in <em>International Congress and Exposition on Noise Control Engineering</em>, Aug. 2020, pp. 4013–4022.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('SKozuka202008Internoise')">bib</a>   <a class="md-button md-button--small" href="https://www.ingentaconnect.com/contentone/ince/incecp/2020/00000261/00000002/art00004">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a><br><div id="SKozuka202008Internoise" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SKozuka202008Internoise</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Kozuka, Shihori and Nakamura, Tomohiko and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Investigation on wavelet basis function of {DNN}-based time domain audio source separation inspired by multiresolution analysis&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;International Congress and Exposition on Noise Control Engineering&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;August&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2020&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;4013--4022&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins> and Hiroshi Saruwatari, “<strong><a href="https://doi.org/10.1109/ICASSP40776.2020.9053934">Time-domain audio source separation based on Wave-U-Net combined with discrete wavelet transform</a></strong>,” in <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, May 2020, pp. 386–390.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura202005ICASSP')">bib</a>   <a class="md-button md-button--small" href="https://arxiv.org/abs/2001.10190">arXiv</a><br><div id="TNakamura202005ICASSP" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TNakamura202005ICASSP</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Saruwatari, Hiroshi&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Time-domain audio source separation based on {Wave-U-Net} combined with discrete wavelet transform&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;IEEE International Conference on Acoustics, Speech, and Signal Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;386--390&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;May&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2020&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/ICASSP40776.2020.9053934&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins> and Hirokazu Kameoka, “<strong><a href="https://doi.org/10.1109/ICASSP.2016.7471723">Shifted and convolutive source-filter non-negative matrix factorization for monaural audio source separation</a></strong>,” in <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, Mar. 2016, pp. 489–493.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura201603ICASSP')">bib</a><br><div id="TNakamura201603ICASSP" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TNakamura201603ICASSP</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Kameoka, Hirokazu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;IEEE International Conference on Acoustics, Speech, and Signal Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;March&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;489--493&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Shifted and convolutive source-filter non-negative matrix factorization for monaural audio source separation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2016&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/ICASSP.2016.7471723&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins> and Hirokazu Kameoka, “<strong><a href="https://doi.org/10.1109/ICASSP.2015.7178344">Lp-norm non-negative matrix factorization and its application to singing voice enhancement</a></strong>,” in <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, Apr. 2015, pp. 2115–2119.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura201503ICASSP')">bib</a><br><div id="TNakamura201503ICASSP" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@INPROCEEDINGS</span><span class="p">{</span><span class="nl">TNakamura201503ICASSP</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Kameoka, Hirokazu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;IEEE International Conference on Acoustics, Speech, and Signal Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Lp-norm non-negative matrix factorization and its application to singing voice enhancement&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2015&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">volume</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">number</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;April&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2115--2119&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/ICASSP.2015.7178344&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins>, Kotaro Shikata, Norihiro Takamune, and Hirokazu Kameoka, “<strong>Harmonic-temporal factor decomposition incorporating music prior information for informed monaural source separation</strong>,” in <em>International Society for Music Information Retrieval Conference</em>, Oct. 2014, pp. 623–628.<br />
<span style="color: var(--md-code-hl-function-color)">[Travel Grant by the Tateishi Science and Technology Foundation]</span><br><a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura201504ISMIR')">bib</a>   <a class="md-button md-button--small" href="http://www.terasoft.com.tw/conf/ismir2014/proceedings/T112_135_Paper.pdf">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a>   <a class="md-button md-button--small" href="demo/HTFD">demo <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M499.1 6.3c8.1 6 12.9 15.6 12.9 25.7v336c0 44.2-43 80-96 80s-96-35.8-96-80 43-80 96-80c11.2 0 22 1.6 32 4.6V147l-256 76.8V432c0 44.2-43 80-96 80S0 476.2 0 432s43-80 96-80c11.2 0 22 1.6 32 4.6V128c0-14.1 9.3-26.6 22.8-30.7l320-96c9.7-2.9 20.2-1.1 28.3 5z"/></svg></span></a><br><div id="TNakamura201504ISMIR" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TNakamura201504ISMIR</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Shikata, Kotaro and Takamune, Norihiro and Kameoka, Hirokazu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;International Society for Music Information Retrieval Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;October&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;623--628&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Harmonic-temporal factor decomposition incorporating music prior information for informed monaural source separation&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2014&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins> and Hirokazu Kameoka, “<strong>Fast signal reconstruction from magnitude spectrogram of continuous wavelet transform based on spectrogram consistency</strong>,” in <em>International Conference on Digital Audio Effects</em>, Sep. 2014, pp. 129–135.<br />
<span style="color: var(--md-code-hl-function-color)">[Travel Grant by the Hara Research Foundation]</span><br><a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura201409DAFx')">bib</a>   <a class="md-button md-button--small" href="http://www.dafx14.fau.de/papers/dafx14_tomohiko_nakamura_fast_signal_reconstructio.pdf">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a>   <a class="md-button md-button--small" href="demo/fastCWT/">demo <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M499.1 6.3c8.1 6 12.9 15.6 12.9 25.7v336c0 44.2-43 80-96 80s-96-35.8-96-80 43-80 96-80c11.2 0 22 1.6 32 4.6V147l-256 76.8V432c0 44.2-43 80-96 80S0 476.2 0 432s43-80 96-80c11.2 0 22 1.6 32 4.6V128c0-14.1 9.3-26.6 22.8-30.7l320-96c9.7-2.9 20.2-1.1 28.3 5z"/></svg></span></a><br><div id="TNakamura201409DAFx" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TNakamura201409DAFx</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Kameoka, Hirokazu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;International Conference on Digital Audio Effects&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;129--135&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Fast signal reconstruction from magnitude spectrogram of continuous wavelet transform based on spectrogram consistency&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2014&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Takuya Higuchi, Hirofumi Takeda, <ins>Tomohiko Nakamura</ins>, and Hirokazu Kameoka, “<strong>A unified approach for underdetermined blind signal separation and source activity detection by multichannel factorial hidden Markov models</strong>,” in <em>INTERSPEECH</em>, Sep. 2014, pp. 850–854.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('THiguchi201409INTERSPEECH')">bib</a>   <a class="md-button md-button--small" href="https://www.isca-speech.org/archive/archive_papers/interspeech_2014/i14_0850.pdf">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a><br><div id="THiguchi201409INTERSPEECH" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">THiguchi201409INTERSPEECH</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Higuchi, Takuya and Takeda, Hirofumi and Nakamura, Tomohiko and Kameoka, Hirokazu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;INTERSPEECH&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;850--854&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;A unified approach for underdetermined blind signal separation and source activity detection by multichannel factorial hidden {Markov} models&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2014&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins>, Hirokazu Kameoka, Kazuyoshi Yoshii, and Masataka Goto, “<strong><a href="https://doi.org/10.1109/ICASSP.2014.6855052">Timbre replacement of harmonic and drum components for music audio signals</a></strong>,” in <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, May 2014, pp. 7520–7524.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura201405ICASSP')">bib</a>   <a class="md-button md-button--small" href="demo/drum_timbre_replacement/">demo <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M499.1 6.3c8.1 6 12.9 15.6 12.9 25.7v336c0 44.2-43 80-96 80s-96-35.8-96-80 43-80 96-80c11.2 0 22 1.6 32 4.6V147l-256 76.8V432c0 44.2-43 80-96 80S0 476.2 0 432s43-80 96-80c11.2 0 22 1.6 32 4.6V128c0-14.1 9.3-26.6 22.8-30.7l320-96c9.7-2.9 20.2-1.1 28.3 5z"/></svg></span></a><br><div id="TNakamura201405ICASSP" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TNakamura201405ICASSP</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Kameoka, Hirokazu and Yoshii, Kazuyoshi and Goto, Masataka&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;IEEE International Conference on Acoustics, Speech, and Signal Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;May&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;7520--7524&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Timbre replacement of harmonic and drum components for music audio signals&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2014&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/ICASSP.2014.6855052&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Takuya Higuchi, Norihiro Takamune, <ins>Tomohiko Nakamura</ins>, and Hirokazu Kameoka, “<strong><a href="https://doi.org/10.1109/ICASSP.2014.6854189">Underdetermined blind separation and tracking of moving sources based on DOA-HMM</a></strong>,” in <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, May 2014, pp. 3215–3219.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('THiguchi201405ICASSP')">bib</a><br><div id="THiguchi201405ICASSP" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">THiguchi201405ICASSP</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Higuchi, Takuya and Takamune, Norihiro and Nakamura, Tomohiko and Kameoka, Hirokazu&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;IEEE International Conference on Acoustics, Speech, and Signal Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;May&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;3215--3219&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Underdetermined blind separation and tracking of moving sources based on DOA-HMM&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2014&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/ICASSP.2014.6854189&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Shigeki Sagayama, <ins>Tomohiko Nakamura</ins>, Eita Nakamura, Yasuyuki Saito, Hirokazu Kameoka, and Nobutaka Ono, “<strong><a href="https://doi.org/10.1121/1.4877848">Automatic music accompaniment allowing errors and arbitrary repeats and jumps</a></strong>,” in <em>Meetings on Acoustics, Acoustic Society of America</em>, May 2014, vol. 21, 35003.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('SSagayama201405ASA')">bib</a><br><div id="SSagayama201405ASA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SSagayama201405ASA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sagayama, Shigeki and Nakamura, Tomohiko and Nakamura, Eita and Saito, Yasuyuki and Kameoka, Hirokazu and Ono, Nobutaka&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Meetings on Acoustics, Acoustic Society of America&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;May&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Automatic music accompaniment allowing errors and arbitrary repeats and jumps&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">volume</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;21, 035003&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2014&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1121/1.4877848&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins>, Eita Nakamura, and Shigeki Sagayama, “<strong>Acoustic score following to musical performance with errors and arbitrary repeats and skips for automatic accompaniment</strong>,” in <em>Sound and Music Computing Conference</em>, Aug. 2013, pp. 299–304.<br />
<span style="color: var(--md-code-hl-function-color)">[Travel Grant by the Telecommunications Advancement Foundation]</span><br><a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura201308SMC')">bib</a>   <a class="md-button md-button--small" href="http://smcnetwork.org/node/1754">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a>   <a class="md-button md-button--small" href="demo/automatic_accompaniment">demo <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M499.1 6.3c8.1 6 12.9 15.6 12.9 25.7v336c0 44.2-43 80-96 80s-96-35.8-96-80 43-80 96-80c11.2 0 22 1.6 32 4.6V147l-256 76.8V432c0 44.2-43 80-96 80S0 476.2 0 432s43-80 96-80c11.2 0 22 1.6 32 4.6V128c0-14.1 9.3-26.6 22.8-30.7l320-96c9.7-2.9 20.2-1.1 28.3 5z"/></svg></span></a><br><div id="TNakamura201308SMC" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TNakamura201308SMC</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Nakamura, Eita and Sagayama, Shigeki&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sound and Music Computing Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;August&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;299--304&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Acoustic score following to musical performance with errors and arbitrary repeats and skips for automatic accompaniment&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2013&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li>Masahiro Nakano, Jonathan Le Roux, Hirokazu Kameoka, <ins>Tomohiko Nakamura</ins>, Nobutaka Ono, and Shigeki Sagayama, “<strong><a href="https://doi.org/10.1109/ASPAA.2011.6082324">Bayesian nonparametric spectrogram modeling based on infinite factorial infinite hidden Markov model</a></strong>,” in <em>IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</em>, Oct. 2011, pp. 325–328.<br />
<a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('MNakano201110WASPAA')">bib</a><br><div id="MNakano201110WASPAA" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">MNakano201110WASPAA</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakano, Masahiro and {Le Roux}, Jonathan and Kameoka, Hirokazu and Nakamura, Tomohiko and Ono, Nobutaka and Sagayama, Shigeki&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;IEEE Workshop on Applications of Signal Processing to Audio and Acoustics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;October&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;325--328&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Bayesian nonparametric spectrogram modeling based on infinite factorial infinite hidden {Markov} model&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2011&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.1109/ASPAA.2011.6082324&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
<li><ins>Tomohiko Nakamura</ins>, Shinji Hara, and Yutaka Hori, “<strong>Local stability analysis for a class of quorum-sensing networks with cyclic gene regulatory networks</strong>,” in <em>SICE Annual Conference</em>, Sep. 2011, pp. 2111–2116.<br />
<span style="color: var(--md-code-hl-function-color)">[SICE Annual Conference 2011 International Award and Finalist of Young Author's Award]</span><br><a class="md-button md-button--small" href="javascript:void(0);" onclick="toggleBib('TNakamura201109SICEAC')">bib</a>   <a class="md-button md-button--small" href="https://ieeexplore.ieee.org/document/6060320">paper <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 464h48v48H64c-35.3 0-64-28.7-64-64V64C0 28.7 28.7 0 64 0h165.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V304h-48V160h-80c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16v384c0 8.8 7.2 16 16 16zm112-112h32c30.9 0 56 25.1 56 56s-25.1 56-56 56h-16v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-16v48h16zm96-80h32c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-32c-8.8 0-16-7.2-16-16V368c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-16v96h16zm80-112c0-8.8 7.2-16 16-16h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V368z"/></svg></span></a><br><div id="TNakamura201109SICEAC" class="bibtex" style="display:none;"><div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TNakamura201109SICEAC</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Nakamura, Tomohiko and Hara, Shinji and Hori, Yutaka&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;SICE Annual Conference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;September&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Local Stability Analysis for a class of Quorum-Sensing Networks with Cyclic Gene Regulatory Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2011&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2111--2116&quot;</span>
<span class="p">}</span>
</code></pre></div></div></li>
</ol>







  
  



  


  



                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021- Tomohoiko Nakamura
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["toc.integrate"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.af256bd8.min.js"></script>
      
    
  </body>
</html>